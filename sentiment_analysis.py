# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gve64GlXQDJcj36bzWYv1Kk1mgHv_eGu
"""

import re
import string

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer

from matplotlib.patches import Ellipse
import matplotlib.transforms as transforms

import numpy as np # Library for linear algebra and math utils


def process_tweet(tweet):
    '''
    Input:
        tweet: a string containing a tweet
    Output:
        tweets_clean: a list of words containing the processed tweet

    '''
    stemmer = PorterStemmer()
    stopwords_english = stopwords.words('english')
    # remove stock market tickers like $GE
    tweet = re.sub(r'\$\w*', '', tweet)
    # remove old style retweet text "RT"
    tweet = re.sub(r'^RT[\s]+', '', tweet)
    # remove hyperlinks
    #tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)
    tweet = re.sub(r'https?://[^\s\n\r]+', '', tweet)
    # remove hashtags
    # only removing the hash # sign from the word
    tweet = re.sub(r'#', '', tweet)
    # tokenize tweets
    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                               reduce_len=True)
    tweet_tokens = tokenizer.tokenize(tweet)

    tweets_clean = []
    for word in tweet_tokens:
        if (word not in stopwords_english and  # remove stopwords
            word not in string.punctuation):  # remove punctuation
            # tweets_clean.append(word)
            stem_word = stemmer.stem(word)  # stemming word
            tweets_clean.append(stem_word)

    return tweets_clean

def lookup(freqs, word, label):
    '''
    Input:
        freqs: a dictionary with the frequency of each pair (or tuple)
        word: the word to look up
        label: the label corresponding to the word
    Output:
        n: the number of times the word with its corresponding label appears.
    '''
    n = 0  # freqs.get((word, label), 0)

    pair = (word, label)
    if (pair in freqs):
        n = freqs[pair]

    return n

# From: https://matplotlib.org/3.1.1/gallery/statistics/confidence_ellipse.html#sphx-glr-gallery-statistics-confidence-ellipse-py


def confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):
    """
    Create a plot of the covariance confidence ellipse of `x` and `y`
    Parameters
    ----------
    x, y : array_like, shape (n, )
        Input data.
    ax : matplotlib.axes.Axes
        The axes object to draw the ellipse into.
    n_std : float
        The number of standard deviations to determine the ellipse's radiuses.
    Returns
    -------
    matplotlib.patches.Ellipse
    Other parameters
    ----------------
    kwargs : `~matplotlib.patches.Patch` properties
    """
    if x.size != y.size:
        raise ValueError("x and y must be the same size")

    cov = np.cov(x, y)
    pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])
    # Using a special case to obtain the eigenvalues of this
    # two-dimensionl dataset.
    ell_radius_x = np.sqrt(1 + pearson)
    ell_radius_y = np.sqrt(1 - pearson)
    ellipse = Ellipse((0, 0),
                      width=ell_radius_x * 2,
                      height=ell_radius_y * 2,
                      facecolor=facecolor,
                      **kwargs)

    # Calculating the stdandard deviation of x from
    # the squareroot of the variance and multiplying
    # with the given number of standard deviations.
    scale_x = np.sqrt(cov[0, 0]) * n_std
    mean_x = np.mean(x)

    # calculating the stdandard deviation of y ...
    scale_y = np.sqrt(cov[1, 1]) * n_std
    mean_y = np.mean(y)

    transf = transforms.Affine2D() \
        .rotate_deg(45) \
        .scale(scale_x, scale_y) \
        .translate(mean_x, mean_y)

    ellipse.set_transform(transf + ax.transData)
    return ax.add_patch(ellipse)

def count_tweets(result, tweets, ys):
    '''
    Input:
        result: a dictionary that will be used to map each pair to its frequency
        tweets: a list of tweets
        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)
    Output:
        result: a dictionary mapping each pair to its frequency
    '''
    for y, tweet in zip(ys, tweets):
        for word in process_tweet(tweet):
            # define the key, which is the word and label tuple
            pair = (word,y)
            
            # if the key exists in the dictionary, increment the count
            if pair in result:
                result[pair] += 1

            # else, if the key is new, add it to the dictionary and set the count to 1
            else:
                result[pair] = 1
    return result

def train_naive_bayes(freqs, train_x, train_y):
    '''
    Input:
        freqs: dictionary from (word, label) to how often the word appears
        train_x: a list of tweets
        train_y: a list of labels correponding to the tweets (0,1)
    Output:
        logprior: the log prior. (equation 3 above)
        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)
    '''
    loglikelihood = {}
    logprior = 0

    ### START CODE HERE ###

    # calculate V, the number of unique words in the vocabulary
    vocab = set([pair[0] for pair in freqs.keys()])
    V = len(vocab)    

    # calculate N_pos, N_neg, V_pos, V_neg
    N_pos = N_neg = 0
    for pair in freqs.keys():
        # if the label is positive (greater than zero)
        if pair[1] > 0:

            # Increment the number of positive words by the count for this (word, label) pair
            N_pos += freqs[pair]

        # else, the label is negative
        else:

            # increment the number of negative words by the count for this (word,label) pair
            N_neg += freqs[pair]
    
    # Calculate D, the number of documents
    D = len(train_y)

    # Calculate D_pos, the number of positive documents
    D_pos = len(list(filter(lambda x: x > 0, train_y)))
    D_neg = len(list(filter(lambda x: x <= 0, train_y)))
             

    # Calculate D_neg, the number of negative documents
   

    # Calculate logprior
    logprior = np.log(D_pos) - np.log(D_neg)
    
    # For each word in the vocabulary...
    for word in vocab:
        # get the positive and negative frequency of the word
        freq_pos = lookup(freqs, word, 1)
        freq_neg = lookup(freqs, word, 0)

        # calculate the probability that each word is positive, and negative
        p_w_pos = (freq_pos + 1) / (N_pos + V)
        p_w_neg = (freq_neg + 1) / (N_neg + V)

        # calculate the log likelihood of the word
        loglikelihood[word] = np.log(p_w_pos / p_w_neg)

    return logprior, loglikelihood

def naive_bayes_predict(tweet, logprior, loglikelihood):
    '''
    Input:
        tweet: a string
        logprior: a number
        loglikelihood: a dictionary of words mapping to numbers
    Output:
        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)

    '''
    # process the tweet to get a list of words
    word_l = process_tweet(tweet)

    # initialize probability to zero
    p = 0

    # add the logprior
    p += logprior

    for word in word_l:

        # check if the word exists in the loglikelihood dictionary
        if word in loglikelihood:
            # add the log likelihood of that word to the probability
            p += loglikelihood[word]

    return p

def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):
    """
    Input:
        test_x: A list of tweets
        test_y: the corresponding labels for the list of tweets
        logprior: the logprior
        loglikelihood: a dictionary with the loglikelihoods for each word
    Output:
        accuracy: (# of tweets classified correctly)/(total # of tweets)
    """
    accuracy = 0  # return this properly

    y_hats = []
    for tweet in test_x:
        # if the prediction is > 0
        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:
            # the predicted class is 1
            y_hat_i = 1
        else:
            # otherwise the predicted class is 0
            y_hat_i = 0

        # append the predicted class to the list y_hats
        y_hats.append(y_hat_i)

    # error is the average of the absolute values of the differences between y_hats and test_y
    error = np.mean(np.absolute(y_hats - test_y))

    # Accuracy is 1 minus the error
    accuracy = 1 - error

    return accuracy

import pandas as pd

training = pd.read_csv('Training.csv',encoding='ISO-8859-1')
train_x = training['X']
train_y = training['Y']

#import package ntlk
import nltk
nltk.download('stopwords')
nltk.download('twitter_samples')

#count frequency
freqs = count_tweets({}, train_x, train_y)
#find logprior and loglihood of train dataset
logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)

print(f'LogPrior is:' logprior)
print(len(loglikelihood))

#load test dataset
test = pd.read_csv('Test.csv',encoding='ISO-8859-1')
test_x = test['x']
test_y = test['y']
#test 
test_naive_bayes(test_x, test_y, logprior, loglikelihood)

#create list of predicted y
y_hats = []
for tweet in test_x:
  if naive_bayes_predict(tweet, logprior, loglikelihood) >0:
    y_hat_i = 1
  else:
    y_hat_i = 0
  y_hats.append(y_hat_i)

#create dataframe with test x, test y_truth and predicted y
result = pd.DataFrame(data = [y_hats, test_y, test_x], index = ['Predicted', 'Truth', 'Tweet']).T

training.head()

train_x = training['X']
train_y = training['Y']

import nltk
nltk.download('stopwords')
nltk.download('twitter_samples')

freqs = count_tweets({}, train_x, train_y)

logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)

logprior

len(loglikelihood)

test = pd.read_csv('Test.csv',encoding='ISO-8859-1')

test_x = test['x']
test_y = test['y']

test_naive_bayes(test_x, test_y, logprior, loglikelihood)

y_hats = []
for tweet in test_x:
  if naive_bayes_predict(tweet, logprior, loglikelihood) >0:
    y_hat_i = 1
  else:
    y_hat_i = 0
  y_hats.append(y_hat_i)

result = pd.DataFrame(data = [y_hats, test_y, test_x], index = ['Predicted', 'Truth', 'Tweet']).T

#create list of cleaned word
clean = []
for text in data['X']:
  word = process_tweet(text)
  clean.extend(word)

pip install wordcloud

#intall wordcloud
pip install wordcloud

#import packages
from wordcloud import WordCloud
import matplotlib.pyplot as plt

all_words = ' '.join([text for text in clean])
wordcloud = WordCloud(width=800, height=500, background_color ='white', random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(8, 8),facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

negative = data[data['Y'] == 0]

clean_neg = []
for text in negative['X']:
  word = process_tweet(text)
  clean_neg.extend(word)

all_words_neg = ' '.join([text for text in clean_neg])
wordcloud = WordCloud(width=800, height=500, background_color ='white', random_state=21, max_font_size=110).generate(all_words_neg)

plt.figure(figsize=(8, 8),facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

positive = data[data['Y'] == 1]
clean_pos = []
for text in positive['X']:
  word = process_tweet(text)
  clean_pos.extend(word)

all_words_pos = ' '.join([text for text in clean_pos])
wordcloud = WordCloud(width=800, height=500, background_color ='white', random_state=21, max_font_size=110).generate(all_words_pos)

plt.figure(figsize=(8, 8),facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

#print('Truth Predicted Tweet')
#for x, y in zip(test_x, test_y):
    #y_hat = naive_bayes_predict(x, logprior, loglikelihood)
    #if y != (np.sign(y_hat) > 0):
        #print('%d\t%0.2f\t%s' % (y, np.sign(y_hat) > 0, ' '.join(
            #process_tweet(x)).encode('ascii', 'ignore')))